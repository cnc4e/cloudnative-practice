issues:
  - title: Checkovによるポリシーチェック
    body: |
      プロジェクト独自のルールや規約（命名規則、タグ付けルール、リージョン制約など）を自動的に検査し、遵守を確保することは重要です。ポリシーチェックツールを使用することで、これらのプロジェクト固有のルールとセキュリティベストプラクティスの両方を自動検査し、コードの品質とセキュリティを向上させることができます。

      # ポリシーチェックの必要性
      - プロジェクト固有のルール（命名規則、タグ付け、リージョン制約）の遵守確認が必要
      - 複数開発者による一貫した設定の維持とコードの統一性の確保
      - 手動によるルール確認は時間がかかり、見落としやヒューマンエラーが発生しやすい
      
      # 解決策
      - **Checkov**: YAMLベースの直感的なカスタムポリシー定義、TerraformとKubernetes両対応（本プラクティス採用）
      - **Trivy**: 高速実行、統合セキュリティスキャナー、カスタムポリシーはOPA/Regoが必要
      - **Terrascan**: OPA/Regoベース、柔軟だが学習コストが高い
      - **KICS**: 独自のクエリ言語、カスタムポリシー作成が複雑
      
      本プラクティスではYAMLでのカスタムポリシー定義が容易で、TerraformとKubernetesの両方に対して統一的なポリシー管理が可能なCheckovを採用します。

      # Checkovによるポリシーチェック
      - Checkovは1000以上のセキュリティ・コンプライアンスチェックを内蔵
      - Terraform、Kubernetes、Docker、CloudFormationなど多様な形式をサポート
      - カスタムポリシーの作成により組織固有のルールを適用可能
      - JSON、SARIF、JUnit形式での結果出力によりCI/CD統合が容易

      ## Checkovのインストールと基本的な使い方

      - [Quick Start - checkov](https://www.checkov.io/1.Welcome/Quick%20Start.html)

      ## 主要なチェック項目
      
      - [kubernetes resource scans - checkov](https://www.checkov.io/5.Policy%20Index/kubernetes.html)
      - [terraform resource scans - checkov](https://www.checkov.io/5.Policy%20Index/terraform.html)      

      ## カスタムポリシーの作成
      
      [Custom Policies Overview - checkov](https://www.checkov.io/3.Custom%20Policies/Custom%20Policies%20Overview.html)

      # プラクティス
      
      ## 基本的なCheckovスキャン
      - checkovをローカル端末にインストール
      - カスタムポリシーを定義し、プロジェクトのルールを自動検査
      - 今回はあえてチェックに`失敗`するようなポリシーを定義（作成したポリシーを別のissueで使います。）
        - codes/checkovディレクトリを作成
        - ディレクトリ内にカスタムポリシーのYAMLを作成
          - ポリシーのIDは`CUSTOM_AWS_PROVIDER_CHECK`にする
          - TerraformのAWSプロバイダーにdefault_regionが`us-west-1`である事を確認
      - 作成したポリシーを使いcheckovを実行しカスタムポリシーの確認結果が`FAILED`になることを確認
        - 対象は`codes/actions/terraform`ディレクトリなど任意
        - `--check "CUSTOM_*"`を付けるとカスタムポリシーのみチェックできる
  - title: CheckovによるポリシーチェックをCIに組み込む
    body: |
      CI/CDパイプラインにポリシーチェックを統合することで、開発者が変更をプッシュする度に自動的にセキュリティとコンプライアンスの検証を行い、問題のあるコードが本番環境にデプロイされることを防げます。

      # CI統合の必要性
      - 開発者の手動実行に依存すると、ポリシーチェックが忘れられる可能性がある
      - プルリクエスト時点での自動検証により、早期に問題を発見・修正できる
      - 継続的なセキュリティとコンプライアンスの維持が可能になる
      - チーム全体での一貫したポリシー適用の強制ができる
      
      # 解決策
      - **GitHub Actions**: 無料プラン利用可能、Checkovの公式アクションあり、設定が簡単（本プラクティス採用）
      - **GitLab CI**: GitLab利用時の選択肢、Dockerイメージでの実行
      - **Jenkins**: 自前構築が必要、柔軟性が高い
      - **CircleCI**: 有料プラン必要、高性能な実行環境
      
      本プラクティスではGitHub Actionsを使用し、プルリクエスト時に自動的にCheckovスキャンを実行する仕組みを構築します。

      # GitHub ActionsでのCheckov統合
      - Checkovには公式のGitHub Actionが提供されている
      - プルリクエスト作成時およびコードプッシュ時に自動実行
      - 結果をプルリクエストのコメントに表示可能
      - 失敗時のワークフロー停止により問題のあるマージを防止

      # プラクティス

      ## 前提

      - Checkovによるポリシーチェック のissueを完了していること

      ## CI統合のためのワークフロー作成
      - `.github/workflows/`ディレクトリに`checkov.yml`ワークフローファイルを作成
      - mainへのプルリクエストをトリガーとしてワークフローを実行
      - [checkov-action](https://github.com/bridgecrewio/checkov-action)を参考にcheckovを実行
        - Terraformディレクトリ（`codes/actions/terraform`等）をスキャン
        - フレームワークは`terraform`
        - カスタムポリシー（`codes/checkov/`）を使用
        - チェックは`CUSTOM_*`のみ

      ## ワークフローのテスト
      - 作業ブランチをリモートにプッシュしPRを作成
      - GitHub Actionsでチェックが自動で行われ**失敗**することを確認
      - `codes/checkov`配下のポリシーを修正しリモートに再プッシュ
      - GitHub Actionsでチェックが自動で行われ**成功**することを確認

      ## 更に発展的なプラクティス
      このプラクティスは余裕がなければやらなくてもいいです。

      - 対象ディレクトリをハードコードではなく変更のあったTerraformディレクトリにする

  - title: AtlantisによるPRベースのterraform plan/apply
    body: |
      Pull Requestベースでのインフラ管理により、レビュープロセスを通じてインフラストラクチャの変更を制御し、変更内容の可視化とガバナンスを強化できます。Atlantisを使用することで、GitOpsワークフローを実現し、Pull Request上で直接terraform planの結果を確認し、レビュー後にterraform applyを実行できます。

      # PRベースインフラ管理の必要性
      - Terraformの実行が個人のローカル環境に依存し、状態管理や権限管理が困難
      - インフラ変更のレビュープロセスが不十分で、予期しない変更が本番環境に反映されるリスク
      - 複数人でのインフラ管理において、実行履歴の追跡と変更の透明性が不足
      - ローカル環境での実行により、一貫性のない実行環境とコンフィグレーションの問題
      
      # 解決策
      - **Atlantis**: GitHubとの統合が優れており、PRコメントでの操作が直感的（本プラクティス採用）
      - **Terraform Cloud**: HashiCorpの公式サービス、リモート実行とstate管理が統合
      - **GitHub Actions**: 自前でワークフローを構築、カスタマイズ性が高い
      - **GitLab CI**: GitLab利用時の選択肢、統合開発環境として利用可能
      
      本プラクティスではGitHub Pull Requestとの親和性が高く、コメントベースでの直感的な操作が可能なAtlantisを採用します。

      # Atlantisによる GitOps ワークフロー
      - Pull Request作成時に自動的にterraform planを実行
      - plan結果をPRコメントに表示し、変更内容を事前に確認可能
      - レビュー承認後、コメントによりterraform applyを実行
      - 実行ログとstate管理を一元化し、トレーサビリティを確保

      ## Atlantisの主要機能
      - **自動plan実行**: PR作成/更新時の自動terraform plan実行
      - **PRコメント統合**: planとapplyをPRコメントから実行
      - **ポリシーチェック**: Conftest等によるOPAポリシー適用
      - **並列実行制御**: 同時実行の制限とロック機能

      # プラクティス

      ## 前提
      - StorageClassを作成してある（issue: クラスタ StorageClassによる動的ボリューム作成）
      - IngressClassを作成してある (issue: クラスタ Ingressによる外部公開)

      ## Atlantisサーバーの構築
      - 公式のドキュメントを参考にAtlantisをKubernetesにデプロイ
        - [Installation Guide | Atlantis](https://www.runatlantis.io/docs/installation-guide.html)
        - 認証情報はARCで作ったPATを使用。**認証情報はGitに含めない**ように注意
        - AtlantisのAWSクレデンシャルはPod IdentityでIAMロールを付与
          - AtlantisのIAMロールからtfbackendへアクセスする。S3のリソースベースポリシーでIAMロールの制限をしている場合、Atlantis用のIAMロールを追加する

      ## 動作確認
      - Terraformのコードを修正したコミットをプッシュし、PRを作成する
      - Atlantisにより自動でplanが実行され結果がPRにコメントされることを確認する

  - title: GitHub Actionsによる定期的なドリフトチェック
    body: |
      Terraformで管理されているインフラストラクチャは、手動変更やAWS側の自動的な変更により、実際のリソース状態とTerraformの状態ファイルに差異（ドリフト）が発生することがあります。定期的なドリフトチェックにより、これらの差異を早期発見し、インフラの整合性を維持できます。

      # ドリフトチェックの必要性
      - 手動でのAWSコンソール操作により、Terraform管理外での変更が発生する可能性
      - AWSサービスの自動更新やメンテナンスによる予期しない設定変更
      - チーム内での緊急対応時の手動変更により、Terraform状態との乖離が発生
      - 長期間運用していると、小さな差異が積み重なり大きな問題に発展するリスク
      
      # 解決策
      - **GitHub Actions**: 無料プラン利用可能、スケジュール実行対応、Terraformとの統合が容易（本プラクティス採用）
      - **AWS Systems Manager**: AWSネイティブサービス、CloudWatchアラーム連携可能
      - **Jenkins**: 自前構築が必要、柔軟性が高いが運用コストが高い
      - **CircleCI**: 有料プラン必要、高性能だが継続的な費用が発生
      
      本プラクティスではGitHub Actionsのscheduled workflowを使用し、定期的にterraform planを実行してドリフトを検出します。

      # GitHub Actionsによるドリフト検出
      - cronスケジュールによる定期実行（毎日、週次等）
      - terraform planの実行結果から差分の有無を判定
      - ドリフト検出時のSlack通知やGitHub Issue自動作成
      - 複数のTerraformディレクトリを並列チェック

      ## ドリフト検出の仕組み
      - `terraform plan -detailed-exitcode`を使用して差分の有無を判定
        - 終了コード0: 差分なし
        - 終了コード2: 差分あり（ドリフト検出）
        - その他: エラー発生
      - 検出された差分内容をSlackやメールで通知
      - 重要度に応じて自動Issue作成や担当者への直接通知

      # プラクティス

      - 定期ドリフトチェックワークフローの作成
        - `.github/workflows/`ディレクトリに`drift-check.yml`ワークフローファイルを作成
        - ワークフローは`arc-runner-set`で実行
        - cronスケジュールでワークフローを定期実行
          - 動作確認時は作業ブランチのpushをトリガーにしてもいい
        - 任意のTerraformディレクトリでドリフトチェックを実行
        - ドリフト検出時は処理を失敗させる
      - arc-runnerのIAMにtfbackendへのアクセス権とAWSへのReadOnlyAccess権を付与
      - tfbackendのリソースポリシーでarc-runnerのIAMロールを許可する

      ## 更に発展的なプラクティス
      このプラクティスは余裕がなければやらなくてもいいです。

      - ドリフト検知時はAWS SNS等を使い通知する
      - ドリフト検知をすべてのTerraformディレクトリに対して行うようにする
  - title: ブランチ戦略
    body: |
      **本issueは内容を確認するだけでクローズしてください。**

      効果的なブランチ戦略により、チーム開発における機能開発、バグ修正、リリース管理を体系化し、コードの品質とリリースの安定性を向上させることができます。適切なブランチ戦略は、並行開発の効率化と競合の最小化を実現します。

      ## ブランチ戦略の必要性

      - **並行開発の管理** - 複数の機能を同時に開発する際の作業分離とコンフリクト回避
      - **品質の担保** - コードレビューとテストによる品質管理の標準化
      - **リリース管理** - リリース準備、ホットフィックス、実験的機能の独立した管理
      - **メインブランチの安定性** - 本番環境にデプロイ可能な状態を常に維持

      ## 主要なブランチ戦略

      参考: [Introduction to GitLab Flow | GitLab](https://docs.gitlab.co.jp/ee/topics/gitlab_flow.html)

      ### GitHub Flow
      - **特徴** - シンプルな構造、軽量、継続デプロイに適している
      - **適用場面** - 小規模チーム、頻繁なデプロイ、Web アプリケーション
      - **ブランチ構成** - mainブランチ + 機能別ブランチ

      ### Git Flow
      - **特徴** - 複雑だが体系的、リリース管理が厳格
      - **適用場面** - 大規模プロジェクト、定期的なリリースサイクル
      - **ブランチ構成** - main, develop, feature, release, hotfix

      ### GitLab Flow
      - **特徴** - GitHub FlowとGit Flowの中間的な位置
      - **適用場面** - 環境ごとのブランチ管理が必要な場合
      - **ブランチ構成** - 環境ブランチ（production, staging）を追加

      ## このプロジェクトでのブランチ戦略

      本プラクティスリポジトリでは**Issue番号ベースのGitHub Flow**を採用しています：

      ### ブランチ命名規則
      - **機能開発** - `{issue番号}-{機能名}` (例: `42-atlantis`, `44-branch-strategy`)

      ### コミットメッセージ規則
      - **形式** - `#{issue番号} {内容}` (例: `#42 Atlantisの設定を追加`)
      - **目的** - Issue番号を含めることでトレーサビリティを確保

      ### ワークフロー
      1. **Issue作成** - GitHub IssueでタスクやバグReportを管理
      2. **ブランチ作成** - Issue番号を含むブランチを作成
      3. **開発・コミット** - Issue番号を含むコミットメッセージで作業
      4. **プルリクエスト** - mainブランチへのマージリクエスト作成
      5. **レビュー・マージ** - コードレビュー後にmainブランチへマージ
  - title: リポジトリ設定
    body: |
      プロジェクトの品質とセキュリティを維持するため、GitHubリポジトリの適切な設定により、コードの保護、レビュープロセスの強制、自動化されたチェックの実行を確保することが重要です。

      # リポジトリ設定の必要性
      - mainブランチへの直接プッシュにより、テストやレビューを経ずに問題のあるコードが反映されるリスク
      - プルリクエストでの必須チェック（テスト、セキュリティスキャン、ポリシーチェック）の未実施
      - 管理者権限の不適切な付与により、保護設定が回避される可能性
      - コードレビューの省略による品質低下とセキュリティ脆弱性の見逃し
      
      # 解決策
      - **保護されたブランチ (Branch Protection Rules)**: 従来の機能、ブランチ単位での保護設定、設定が簡単で広く使用されている
      - **ルールセット (Repository Rulesets)**: 新しい機能、より柔軟で統合的なリポジトリルール管理、複数ブランチやタグに対する統一ルール適用が可能（本プラクティス採用）
      
      本プラクティスでは新しいルールセット機能を使用し、より柔軟で包括的なリポジトリの保護設定を行います。

      # ルールセットによるリポジトリ保護
      
      [ルールセット](https://docs.github.com/ja/repositories/configuring-branches-and-merges-in-your-repository/managing-rulesets/about-rulesets)は、従来の保護されたブランチ機能を拡張し、より柔軟で包括的なリポジトリルール管理を提供します。
      
      ## ルールセットの主要機能

      - [Available rules for rulesets](https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/managing-rulesets/available-rules-for-rulesets)。以下一部抜粋
        - **Require a pull request before merging**: プルリクエストでの必須レビューと承認者数の設定
        - **Require status checks to pass before merging**: CI/CDチェックの成功を必須条件に設定
        - **Block force pushes**: 強制プッシュの禁止
        - **Restrict deletions**: ブランチ削除の制限

      # プラクティス
      
      ルールセットを設定してリポジトリを保護します。
      ルールセットを作成する際は ArgoCD ImageUpdate の認証方法に応じて設定を行います。

      - PAT & GitHub Apps 認証の場合
        - リポジトリの削除を防ぐルールを設定
          - ルールセット名: `main-branch-delete-protection`
          - Enforcement status: `Active`に設定
          - Bypass list: `Repository admin`を指定
          - 対象ブランチ: `main`ブランチを指定
          - 以下のルールを有効化
            - "Restrict deletions"
      - GitHub Apps認証の場合のみ（バイパスを設定しないとArgoCD ImageUpdaterによる自動更新ができなくなるため）
        - リポジトリの削除・mainブランチへの直接プッシュを防ぐルールを設定
          - ルールセット名: `main-branch-push-protection`
          - Enforcement status: `Active`に設定
          - Bypass list: ArgoCDで使用したGitHub Appsを指定
          - 対象ブランチ: `main`ブランチを指定
          - 以下のルールを有効化
            - "Restrict deletions"
            - "Block force pushes"
            - "Require a pull request before merging"
              - Required approvals: 1
              - "Dismiss stale pull request reviews when new commits are pushed"を有効化

  - title: Terraform構成管理
    body: |
      **本issueは内容を確認するだけでクローズしてください。**

      Terraformプロジェクトが成長するにつれて、リソース定義の増加や複雑な依存関係により、コードの管理が困難になります。適切な構成管理により、保守性、再利用性、可読性を向上させ、チーム開発での効率的なインフラストラクチャ管理を実現できます。

      # Terraform構成管理の必要性
      - 単一ファイルでの大量リソース定義により、可読性と保守性が著しく低下
      - 環境間（dev、staging、prod）での設定差異の管理が複雑化
      - チーム開発において、同時変更による競合や影響範囲の把握が困難
      - リソース変更時の影響範囲が不明確で、予期しない副作用のリスク

      # 解決策
      - **ディレクトリ分割**: リソース種別や機能別にファイルを分離、シンプルで理解しやすい、小規模〜中規模プロジェクトに適している（本プロジェクト採用）
      - **モジュール化**: 再利用可能なコンポーネントとして抽象化、複雑な構成に対応可能、大規模プロジェクトや複数環境管理に適している
      - **Terraformワークスペース**: 同一構成で複数環境を管理、環境別の状態分離が可能、シンプルな環境管理に適している

      本プロジェクトでは初心者でもわかりやすいディレクトリ分割のアプローチを採用しています。

      # ディレクトリ分割アプローチ
      - リソースの責務や機能に基づくファイル分離
      - 依存関係の明確化とdata sourceによるリソース参照
      - コード自体の複雑性は低く、初心者にも理解しやすい

      ## 基本的なディレクトリ構成（環境軸）
      - 各環境ごとにディレクトリを分け、さらにモジュールごとにサブディレクトリを作成
      - 環境を増やす場合は、既存の環境ディレクトリをコピーして作成できる

      ```
      codes/
      ├── dev/          # 開発環境
      │   ├── network/      # ネットワーク関連モジュール
      │   ├── eks/          # EKS関連モジュール
      │   └── security/     # セキュリティ関連モジュール
      └── prd/          # 本番環境
          ├── network/      # ネットワーク関連モジュール
          ├── eks/          # EKS関連モジュール
          └── security/     # セキュリティ関連モジュール
      ```

      ## モジュール軸のディレクトリ構成
      - 各モジュールごとにディレクトリを分け、さらに環境ごとにサブディレクトリを作成
      - リソースを定義したコードの階層が近いため横ぐしの修正がしやすい

      ```
      codes/
      ├── network/    # ネットワーク関連モジュール
      │   ├── dev/       # 開発環境
      │   └── prd/       # 本番環境
      ├── eks/        # EKS関連モジュール
      │   ├── dev/       # 開発環境
      │   └── prd/       # 本番環境
      └── security/   # セキュリティ関連モジュール
          ├── dev/       # 開発環境
          └── prd/       # 本番環境
      ```

      # モジュール化アプローチ
      - 再利用可能なコンポーネントとしての抽象化
      - input変数とoutput値による明確なインターフェース定義
      - 複数環境での設定値の差分管理
      - コードを再利用できるが複雑性が増すため、チームのスキルレベルに応じて導入を検討

      ## モジュール化の構成例
      ```
      codes/
      ├── modules/        # 共通モジュール定義
      │   ├── network/      # ネットワーク関連モジュール(呼び出し先)
      │   ├── eks/          # EKS関連モジュール(呼び出し先)
      │   └── security/     # セキュリティ関連モジュール(呼び出し先)
      └── environments/   # 環境別設定
          ├── dev/          # 開発環境
          │   ├── network/    # ネットワーク関連モジュール(呼び出し元)
          │   ├── eks/        # EKS関連モジュール(呼び出し元)
          │   └── security/   # セキュリティ関連モジュール(呼び出し元)
          └── prd/          # 本番環境
      ```

      # Terraformワークスペースアプローチ
      - 同一構成で複数環境を管理、環境別状態ファイル分離
      - terraform.workspace変数による環境固有設定の制御
      - シンプルな環境切り替えと軽量な環境管理手法

  - title: K8sマニフェスト構成管理
    body: |
      **本issueは内容を確認するだけでクローズしてください。**

      Kubernetesアプリケーションが成長するにつれて、マニフェストファイルの数が増加し、管理が複雑になります。適切な構成管理により、保守性、可読性、再利用性を向上させ、チーム開発での効率的なKubernetesアプリケーション管理を実現できます。

      # K8sマニフェスト構成管理の必要性
      - 単一ファイルでの大量リソース定義により、可読性と保守性が著しく低下
      - 環境間（dev、staging、prod）でのマニフェスト差異の管理が複雑化
      - アプリケーション別、機能別のリソースが混在し、影響範囲の把握が困難
      - 同じような設定の重複によるメンテナンスコストの増大

      # 解決策
      - **ディレクトリ分割**: アプリケーション・機能別にファイルを分離、シンプルで理解しやすい、小規模〜中規模に適している（本プロジェクト採用）
      - **Kustomize**: ベースマニフェストから環境別の差分を管理、YAML重複を削減、Kubernetesネイティブツール
      - **Helm**: テンプレート化による高度な抽象化、複雑な設定管理が可能、大規模プロジェクトに適している
      - **ArgoCD ApplicationSets**: 複数環境への一括デプロイ、GitOpsパターンでの一元管理

      本プロジェクトでは初心者でもわかりやすいディレクトリ分割のアプローチを採用しています。

      # ディレクトリ分割アプローチ
      - アプリケーションや機能に基づくファイル分離
      - 環境別の設定値は同一ファイル内で管理
      - リソース種別ごとのファイル分割により、変更影響範囲を限定

      ## 基本的なディレクトリ構成（アプリケーション軸）
      - 各アプリケーション（フロントエンド、バックエンド）ごとにディレクトリを分割
      - リソース種別ごとにファイルを分離し、管理しやすい構成にする

      ```
      codes/
      ├── backend/
      │   ├── k8s/
      │   │   ├── deployment.yaml    # Deployment定義
      │   │   ├── service.yaml       # Service定義
      │   │   ├── configmap.yaml     # ConfigMap定義
      │   │   └── ingress.yaml       # Ingress定義
      │   └── app/                   # アプリケーションソースコード
      └── frontend/
          ├── k8s/
          │   ├── deployment.yaml    # Deployment定義
          │   ├── service.yaml       # Service定義
          │   ├── configmap.yaml     # ConfigMap定義
          │   └── ingress.yaml       # Ingress定義
          └── app/                   # アプリケーションソースコード
      ```

      # Kustomizeアプローチ
      - ベースマニフェストから環境別の差分（patch）を適用
      - YAML重複を削減し、DRY原則に従った構成管理
      - Kubernetesネイティブツールのため、追加ツールのインストール不要

      ## Kustomize構成例
      ```
      codes/
      ├── base/                     # 共通ベースマニフェスト
      │   ├── deployment.yaml
      │   ├── service.yaml
      │   └── kustomization.yaml
      └── overlays/                 # 環境別差分
          ├── dev/
          │   ├── kustomization.yaml
          │   └── patch-replica.yaml
          └── prod/
              ├── kustomization.yaml
              └── patch-replica.yaml
      ```

      # Helmアプローチ
      - テンプレート化による高度な抽象化と動的設定生成
      - values.yamlによる環境別設定管理
      - Helm Chartの再利用性と配布可能性

      ## Helm構成例
      ```
      codes/
      └── helm-charts/
          └── myapp/
              ├── Chart.yaml          # Chart情報
              ├── values.yaml         # デフォルト設定値
              ├── values-dev.yaml     # 開発環境設定
              ├── values-prod.yaml    # 本番環境設定
              └── templates/          # テンプレートファイル
                  ├── deployment.yaml
                  ├── service.yaml
                  └── ingress.yaml
      ```

  - title: EKSクラスタのバージョンアップ
    body: |
      EKSクラスタは定期的に新しいKubernetesバージョンがリリースされ、セキュリティ修正や機能改善が提供されます。クラスタのバージョンを最新に保つことで、セキュリティの向上と新機能の活用が可能になります。

      # EKSバージョンアップの必要性
      - 古いKubernetesバージョンのサポート終了によるセキュリティリスクの増大
      - 新機能やAPIの利用不可による開発効率の低下
      - AWSによるEKSコントロールプレーンの強制アップグレードを避けるための計画的な更新
      - セキュリティパッチや重要な修正の適用により、クラスタの安定性向上

      参考
      - [EKS の Kubernetes バージョンライフサイクルを理解する](https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/kubernetes-versions.html)

      # 解決策

      K8sクラスタの代表的なバージョンアップ戦略は以下2つがあります。

      - **インプレースアップグレード**: 既存のクラスタを直接更新する方式、シンプルで費用効率が良い（本プラクティス採用）
      - **ブルーグリーンアップグレード**: 新しいクラスタを作成して段階的に移行する方式、リスクは低いが費用と複雑性が増加

      本プラクティスでは運用の簡素化とコスト効率を重視し、インプレースアップグレード方式を採用します。

      ## インプレースアップグレード
      - **特徴**: 既存のクラスタ上で直接バージョンアップを実行
      - **利点**: 
        - コスト効率が良い（追加リソース不要）
        - シンプルな手順で実行可能
        - DNSやロードバランサーの設定変更が不要
      - **考慮点**: 
        - アップグレード中の一時的なサービス停止の可能性
        - 問題発生時のロールバックが困難

      ## ブルーグリーンアップグレード
      - **特徴**: 新しいバージョンのクラスタを並行して作成し、段階的に移行
      - **利点**: 
        - 問題発生時の迅速なロールバックが可能
        - 本番環境への影響を最小限に抑制
        - アップグレードテストを本格的に実施可能
      - **考慮点**: 
        - 2倍のリソース費用が一時的に発生
        - データベースやストレージの移行が複雑
        - DNS切り替えやトラフィック制御が必要

      # EKS AutoModeでのインプレースアップグレード
      - EKS AutoModeではコントロールプレーンのアップグレード後、ノードは自動的に新しいバージョンに更新される
      - アップグレード前の事前チェックと互換性確認が重要
      - ローリングアップデートにより、サービス停止時間を最小限に抑制

      ## バージョンアップの考慮事項
      - **Kubernetesバージョン互換性**: APIの非推奨化と削除への対応
      - **アドオンの互換性**: AWS Load Balancer Controller、EBS CSI Driver等の対応バージョン確認
      - **アプリケーションの互換性**: デプロイ済みアプリケーションの動作確認
      - **ダウンタイム**: ローリングアップデートによるサービス継続性の確保
      - **バージョンアップ後の動作確認**: すべてをバージョンアップした後にクラスタ機能の動作確認

      ## バージョンアップの大まかな手順
      - 事前準備
        - 最新バージョンのリリースノートで影響を確認する。[標準サポートの Kubernetes バージョンのリリースノートを確認する](https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/kubernetes-versions-standard.html)
        - 各種アドオンについて、バージョンアップ後のクラスタに対応しているか確認する
      - バージョンアップの実施
        - kubectlコマンドをバージョンアップ
        - EKSコントロールプレーンのバージョンアップ
        - （ノードのバージョンアップ）
        - 各種アドオンのバージョンアップ

      # プラクティス

      ## 事前準備
      - 最新バージョンのリリースノートで影響を確認する
        - [標準サポートの Kubernetes バージョンのリリースノートを確認する](https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/kubernetes-versions-standard.html)
        - APIの非推奨化や削除、重要な変更点をチェック
      - 各種アドオンについて、バージョンアップ後のクラスタに対応しているか確認する
        - 現在のEKSクラスタバージョンを確認する
          ```bash
          aws eks describe-cluster --name <cluster-name> --query 'cluster.version'
          ```
        - 利用可能なEKSバージョンを確認する
          ```bash
          aws eks describe-addon-versions --kubernetes-version <current-version>
          ```
        - クラスタにインストールされているアドオンとそのバージョンを確認する
          ```bash
          aws eks list-addons --cluster-name <cluster-name>
          ```
        - ArgoCD等、EKSアドオン以外もバージョンアップ後のクラスタバージョンに対応しているか確認する

      ## バージョンアップの実施
      - kubectlコマンドをバージョンアップ
        - アップグレード予定のEKSバージョンに対応するkubectlをインストール
        - kubectl バージョンは、クラスター バージョンの1つ上または下のマイナー バージョン内である必要がある
      - EKSコントロールプレーンのバージョンアップ
        - TerraformでEKSクラスタのバージョンを次のマイナーバージョンに更新する
        - 一度に複数のマイナーバージョンをスキップすることは推奨されない
        - `terraform plan`で変更内容を確認し、`terraform apply`でアップグレードを実行
        - アップグレード進行状況をAWSコンソールまたはCLIで監視する
          ```bash
          aws eks describe-update --name <cluster-name> --update-id <update-id>
          ```
      - ノードのバージョンアップ
        - EKS AutoModeの場合、コントロールプレーンアップグレード後に自動的に実行される
        - ノードのバージョンアップ完了まで待機し、進行状況を監視する
      - EKSアドオンのバージョンアップ
        - EBS CSI Driver、AWS Load Balancer Controller等のアドオンを最新バージョンにアップグレード
        - アドオンごとに互換性を確認してからアップグレードを実行
      - その他アドオンのバージョンアップ
        - ArgoCD等

      ## バージョンアップ後の動作確認
      - クラスタのステータスがACTIVEであることを確認
        ```bash
        aws eks describe-cluster --name <cluster-name> --query 'cluster.status'
        ```
      - ノードのKubeletバージョンがアップグレードされていることを確認
        ```bash
        kubectl get nodes -o wide
        ```
      - 既存のPodとサービスが正常に動作していることを確認
        ```bash
        kubectl get pods --all-namespaces
        kubectl get svc --all-namespaces
        ```
      - アプリケーションが正常にアクセス可能であることを確認

  - title: Veleroを使ったバックアップとリストア
    body: |
      Kubernetesクラスタ上のアプリケーションとデータの定期的なバックアップは、災害復旧、誤操作からの回復、環境間でのデータ移行において重要です。DBなどK8s外部に保管したデータは外部サービスの機能でバックアップするのが一般的です。また、K8sリソースはマニフェストをapplyすれば再作成可能です。しかし、DVPでデプロイしたPVのボリュームやマニフェストにしていないK8sリソース（例えば、Argoのadminパスワードなど動的に作成されるリソース）は対策がいります。Veleroを使用することで、クラスタリソースとPersistent Volumeの統合的なバックアップとリストアを実現できます。

      # バックアップの必要性
      - 人的ミスによるリソース削除や設定変更からの迅速な復旧
      - ハードウェア障害やクラスタ障害時のデータ保護とサービス継続性の確保
      - 開発・ステージング環境への本番データの安全な複製
      - クラスタ移行やマルチクラスタ環境でのデータ同期

      # 解決策
      - **Velero**: Kubernetesネイティブなバックアップツール、リソースとボリュームの統合管理、災害復旧に特化（本プラクティス採用）
      - **etcdスナップショット**: コントロールプレーンのバックアップ、Kubernetesクラスタ全体の状態保存、EKSでは利用で管理不可
      - **アプリケーション固有ツール**: データベースダンプ等、アプリケーション特化の手法、細かい制御が可能

      本プラクティスではKubernetes環境での包括的なバックアップとリストアが可能なVeleroを採用します。

      # Veleroによるバックアップとリストア
      - **リソースバックアップ**: Kubernetesリソース（Deployment、Service、ConfigMap等）のYAML定義
      - **ボリュームバックアップ**: Persistent VolumeのデータをCSIスナップショットまたはresticで保護
      - **選択的バックアップ**: 名前空間、ラベル、リソース種別による柔軟なバックアップ対象選択
      - **スケジュールバックアップ**: cron形式での定期的な自動バックアップ実行

      ## Veleroの主要機能
      - **災害復旧**: クラスタ全体または特定の名前空間の完全復旧
      - **クラスタ移行**: 異なるクラスタ間でのリソースとデータの移行
      - **バックアップ検証**: バックアップの整合性確認と復旧テスト
      - **クラウド統合**: AWS S3、Azure Blob、GCS等のオブジェクトストレージとの連携

      ## VeleroのPersistent Volumeバックアップ方式
      - **CSIスナップショット**: AWS EBS、Azure Disk等のクラウドネイティブスナップショット機能を使用
      - **Restic統合**: ファイルシステムレベルでのバックアップ、クラウドに依存しない汎用的な方式
      - **File System Backup（Node Agent）**: Velero 1.10以降のrestic後継機能、より高性能な統合バックアップ

      本プラクティスではAWS EBSのCSIスナップショット機能を使用します。

      # プラクティス

      - Veleroのクラスタにインストール
        - 前提AWSリソース
          - バックアップ用のS3バケット
          - Veleroサーバー用 IAMロールおよび Pod Identity
          - EKSアドオン（snapshot-controller）の有効化
        - 前提K8sリソース
          - VolumeSnapshotClassの作成
        - Veleroサーバーのデプロイ [参考](https://velero.io/docs/v1.16/basic-install/)
        - Velero CLIのインストール
      - テスト用アプリケーションを使いVeleroのバックアップ/リストアを検証
        - PVCを使いPVを持つアプリケーションをデプロイ
          - 適当なデータをPVに保管
        - Veleroを使ってバックアップを作成
        - テスト用アプリケーションを削除
        - Veleroを使ってリストアを実行
          - リストア後、PVに保管したデータが復元されていることを確認